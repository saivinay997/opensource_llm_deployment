#!/usr/bin/env python3
"""
Memory Check Script for Large Model Deployment
Checks system resources and provides recommendations for model deployment
"""

import sys
from utils import check_system_resources, get_model_recommendations, is_large_model

def main():
    """Main function for memory check."""
    print("üîç System Resource Check")
    print("=" * 50)
    
    # Check system resources
    resources = check_system_resources()
    
    if not resources:
        print("‚ùå Error: Could not check system resources")
        return
    
    # Display memory status
    memory = resources["memory"]
    print(f"üìä Memory Status:")
    print(f"   Total RAM: {memory['total_gb']:.1f} GB")
    print(f"   Available: {memory['available_gb']:.1f} GB")
    print(f"   Used: {memory['used_gb']:.1f} GB ({memory['percent']:.1f}%)")
    
    # Display CPU status
    cpu = resources["cpu"]
    print(f"\nüñ•Ô∏è  CPU Status:")
    print(f"   Cores: {cpu['cores']}")
    print(f"   Usage: {cpu['usage_percent']:.1f}%")
    
    # Display disk status
    disk = resources["disk"]
    print(f"\nüíæ Disk Status:")
    print(f"   Total: {disk['total_gb']:.1f} GB")
    print(f"   Free: {disk['free_gb']:.1f} GB")
    
    # Recommendations
    print(f"\nüí° Recommendations:")
    
    if memory['total_gb'] < 16:
        print("   ‚ùå Insufficient RAM for large models")
        print("   üí° Recommended models: gpt2, distilgpt2, microsoft/DialoGPT-medium")
    elif memory['total_gb'] < 32:
        print("   ‚ö†Ô∏è  Limited RAM for 20B models")
        print("   üí° Consider smaller models or ensure no other processes running")
        print("   üí° Recommended: microsoft/DialoGPT-medium, gpt2")
    else:
        print("   ‚úÖ Sufficient RAM for large models")
        print("   üí° You can try facebook/opt-1.3b or other large models")
    
    if disk['free_gb'] < 50:
        print("   ‚ö†Ô∏è  Limited disk space for model caching")
        print("   üí° Clear some space or use smaller models")
    
    # Get model recommendations
    recommendations = get_model_recommendations(memory['available_gb'])
    print(f"\nüöÄ Recommended Models for {memory['available_gb']:.1f}GB RAM:")
    for model in recommendations:
        print(f"   ‚Ä¢ {model}")
    
    print(f"\nüöÄ Model Recommendations by RAM:")
    print(f"   < 8GB:  gpt2, distilgpt2")
    print(f"   8-16GB: microsoft/DialoGPT-medium, EleutherAI/gpt-neo-125M")
    print(f"   16-32GB: facebook/opt-350m, microsoft/DialoGPT-large")
    print(f"   > 32GB: facebook/opt-1.3b, other large models")

def check_model_requirements(model_name):
    """Check specific requirements for a model."""
    print(f"\nüéØ Model Requirements Check: {model_name}")
    print("=" * 50)
    
    if is_large_model(model_name):
        print("‚ö†Ô∏è  Large Model Detected!")
        print("   ‚Ä¢ Requires 32GB+ RAM")
        print("   ‚Ä¢ CPU-only deployment recommended")
        print("   ‚Ä¢ May take 10-30 minutes to load")
        print("   ‚Ä¢ Consider using smaller models for testing")
    elif "7b" in model_name.lower():
        print("‚ö†Ô∏è  Medium-Large Model")
        print("   ‚Ä¢ Requires 16GB+ RAM")
        print("   ‚Ä¢ CPU deployment possible")
    else:
        print("‚úÖ Standard Model")
        print("   ‚Ä¢ Should work on most systems")
        print("   ‚Ä¢ 4-8GB RAM sufficient")

if __name__ == "__main__":
    main()
    
    if len(sys.argv) > 1:
        model_name = sys.argv[1]
        check_model_requirements(model_name)
    else:
        print(f"\nüí° Usage: python memory_check.py <model_name>")
        print(f"   Example: python memory_check.py facebook/opt-1.3b")
